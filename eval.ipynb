{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T12:36:02.868259Z",
     "iopub.status.busy": "2024-12-17T12:36:02.866607Z",
     "iopub.status.idle": "2024-12-17T12:36:05.484134Z",
     "shell.execute_reply": "2024-12-17T12:36:05.483605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import os for file operations\n",
    "import os\n",
    "\n",
    "# Import numpy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Import random for random number generation\n",
    "import random\n",
    " \n",
    "# Import PIL and matplotlib for image operations\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import torch for GPU Data processing\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "\n",
    "# Import torchvision for  Data transformation\n",
    "from torchvision import transforms\n",
    "import torchvision.models as torchmodels\n",
    "\n",
    "\n",
    "# Import tqdm for visualizing progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import xmltodict for parsing xml files\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T12:36:05.485827Z",
     "iopub.status.busy": "2024-12-17T12:36:05.485595Z",
     "iopub.status.idle": "2024-12-17T12:36:05.560852Z",
     "shell.execute_reply": "2024-12-17T12:36:05.559995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060\n",
      "Device set to: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check and print GPU information if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device set to: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T12:36:05.586950Z",
     "iopub.status.busy": "2024-12-17T12:36:05.586591Z",
     "iopub.status.idle": "2024-12-17T12:36:05.591960Z",
     "shell.execute_reply": "2024-12-17T12:36:05.591573Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(data_dir, train_ratio=0.7, val_ratio=0.15, num_classes=None, seed=None):\n",
    "\n",
    "    # Set seed for reproducibility if provided\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    # Otherwise, set a random seed\n",
    "    else:\n",
    "        random.seed()\n",
    "\n",
    "    # Get the sorted list of classifications\n",
    "    classes = sorted(os.listdir(data_dir))\n",
    "\n",
    "    # Check if a specified number of classes is provided\n",
    "    if num_classes is not None:\n",
    "        # Make an upper bound to the number of classes\n",
    "        classes = classes[:num_classes]\n",
    "\n",
    "    # Create a dictionary mapping the class breed names to integers\n",
    "    breed_dict = {breed: i for i, breed in enumerate(classes)}\n",
    "\n",
    "    # Initialize the 3 splits sets\n",
    "    train_set = []\n",
    "    val_set = []\n",
    "    test_set = []\n",
    "\n",
    "    # Iterate over the classes\n",
    "    for breed in classes:\n",
    "        # Get the list of images for the current class\n",
    "        img_list = os.listdir(os.path.join(data_dir, breed))\n",
    "        # Make sure only images are captured\n",
    "        img_list = [img for img in img_list if img.endswith('.jpg')]\n",
    "        \n",
    "        # Shuffle the list of images\n",
    "        random.shuffle(img_list)\n",
    "\n",
    "        # Calculate the split indexes\n",
    "        num_train = int(len(img_list) * train_ratio)\n",
    "        num_val = int(len(img_list) * val_ratio)\n",
    "        num_test = len(img_list) - num_train - num_val\n",
    "        \n",
    "        # Split the images into the 3 sets\n",
    "        train_set += [(os.path.join(data_dir, breed, img), breed_dict[breed]) for img in img_list[:num_train]]\n",
    "        val_set += [(os.path.join(data_dir, breed, img), breed_dict[breed]) for img in img_list[num_train:num_train + num_val]]\n",
    "        test_set += [(os.path.join(data_dir, breed, img), breed_dict[breed]) for img in img_list[num_train + num_val:]]\n",
    "\n",
    "    return train_set, val_set, test_set, breed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T12:36:05.593327Z",
     "iopub.status.busy": "2024-12-17T12:36:05.593166Z",
     "iopub.status.idle": "2024-12-17T12:36:05.598033Z",
     "shell.execute_reply": "2024-12-17T12:36:05.597612Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataLoaderClassification(Dataset):\n",
    "    def __init__(self, image_set, breed_dict, transform=None):\n",
    "        \n",
    "        self.image_set = image_set\n",
    "        self.transform = transform\n",
    "        self.breed_dict = breed_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_set)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_set[idx][0]\n",
    "        \n",
    "        # Make sure the image file exists\n",
    "        if not os.path.exists(img_path):\n",
    "            raise FileNotFoundError(f\"Image file not found: {img_path}\")\n",
    "    \n",
    "        # Open the image\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Get the respective bounding box file path\n",
    "        bbox_path = img_path.replace('Images', 'Annotation').removesuffix('.jpg') \n",
    "        \n",
    "        # Check if the bounding box file exists\n",
    "        if not os.path.exists(bbox_path):\n",
    "            raise FileNotFoundError(f\"Bounding box file not found: {bbox_path}\")\n",
    "        \n",
    "        # Parse the bounding box XML file\n",
    "        with open(bbox_path) as fd:\n",
    "            doc = xmltodict.parse(fd.read())\n",
    "        \n",
    "        # Handle multiple objects in the annotation\n",
    "        objects = doc['annotation']['object']\n",
    "        if isinstance(objects, list):\n",
    "            bndbox = objects[0]['bndbox']  # Use the first object's bounding box\n",
    "        else:\n",
    "            bndbox = objects['bndbox']\n",
    "        \n",
    "        xmin = int(bndbox['xmin'])\n",
    "        ymin = int(bndbox['ymin'])\n",
    "        xmax = int(bndbox['xmax'])\n",
    "        ymax = int(bndbox['ymax'])\n",
    "            \n",
    "        # Crop the image using the bounding box coordinates\n",
    "        img = img.crop((xmin, ymin, xmax, ymax))\n",
    "        \n",
    "        # Apply transformation if any\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Get the label from the image path using the breed_to_label dictionary\n",
    "        label = self.breed_dict[img_path.split('/')[-2]]\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T12:36:05.599448Z",
     "iopub.status.busy": "2024-12-17T12:36:05.599267Z",
     "iopub.status.idle": "2024-12-17T12:36:05.729320Z",
     "shell.execute_reply": "2024-12-17T12:36:05.728899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in the dataset: 20580\n",
      "Samples in the training set: 14355\n",
      "Samples in the validation set: 3025\n",
      "Samples in the test set: 3200\n",
      "Data split successful\n"
     ]
    }
   ],
   "source": [
    "# Define the mean and std standards to be used consistently \n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define the transformation to be applied on the training set\n",
    "train_transform = transforms.Compose([\n",
    "    # We perform data augmentation in the training set to reduce overfitting\n",
    "    #transforms.RandomResizedCrop(128),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Lambda(lambda img: img.convert('RGB')),  \n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image to the defined mean and std\n",
    "    #transforms.Normalize(mean, std)\n",
    "])\n",
    "# Note we resize before converting to tensor, since in PIL format it is easier to resize and less computationally expensive\n",
    "\n",
    "\n",
    "# Define the transformation to be applied on the validation set\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Lambda(lambda img: img.convert('RGB')), \n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "\n",
    "# Define the dataset image directory\n",
    "directory = '/opt/nfs/shared/images/ImageNetDogs/Images'\n",
    "\n",
    "# Split the dataset using the split_dataset function\n",
    "train_set, val_set, test_set, breed_dict = split_dataset(directory, train_ratio=0.7, val_ratio=0.15)\n",
    "\n",
    "# Set the number of classes\n",
    "num_classes = len(breed_dict)\n",
    "\n",
    "# Reverse dictionary to map labels to breed names\n",
    "label_dict = {label: breed for breed, label in breed_dict.items()}\n",
    "\n",
    "# Initialize the DataLoader class instances for the train, val and test sets\n",
    "train_loader = DataLoaderClassification(train_set, breed_dict, transform=train_transform)\n",
    "val_loader = DataLoaderClassification(val_set, breed_dict, transform=val_transform)\n",
    "test_loader = DataLoaderClassification(test_set, breed_dict, transform=val_transform)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Create the DataLoader objects for the train, val and test sets\n",
    "train_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_loader, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_loader, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Verify the data loaders\n",
    "total_samples = int(os.popen(f'find {directory} -type f | wc -l').read().strip())\n",
    "train_samples = len(train_loader.dataset)\n",
    "val_samples = len(val_loader.dataset)\n",
    "test_samples = len(test_loader.dataset)\n",
    "print(f\"Total samples in the dataset: {total_samples}\")\n",
    "print(f\"Samples in the training set: {train_samples}\")\n",
    "print(f\"Samples in the validation set: {val_samples}\")\n",
    "print(f\"Samples in the test set: {test_samples}\")\n",
    "\n",
    "if total_samples == (train_samples + val_samples + test_samples):\n",
    "    print(\"Data split successful\")\n",
    "else:\n",
    "    print(\"Data split unsuccessful\")\n",
    "    print(f\"Total samples is meant to be {total_samples} but got {train_samples + val_samples + test_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T12:36:05.730829Z",
     "iopub.status.busy": "2024-12-17T12:36:05.730634Z",
     "iopub.status.idle": "2024-12-17T12:36:05.734226Z",
     "shell.execute_reply": "2024-12-17T12:36:05.733836Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_loader)\n",
    "    val_accuracy = correct_val / total_val\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T12:36:05.735715Z",
     "iopub.status.busy": "2024-12-17T12:36:05.735490Z",
     "iopub.status.idle": "2024-12-17T12:36:06.510610Z",
     "shell.execute_reply": "2024-12-17T12:36:06.510162Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 63\u001b[0m\n\u001b[1;32m     53\u001b[0m models_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Model/Simple-CNN/simple-model-best.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Model/Improved-CNN/improved-model-best.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Model/VGG16/vgg16-2-model-best.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     60\u001b[0m ]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Assuming test_loader and device are already defined\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[43mcompare_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36mcompare_models\u001b[0;34m(models_paths, test_loader)\u001b[0m\n\u001b[1;32m     12\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Evaluate the model using the evaluate_model function\u001b[39;00m\n\u001b[1;32m     17\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, criterion, device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compare_models(models_paths, test_loader):\n",
    "    # Define the criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Create a list to store accuracy and loss for each model\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    # Iterate over the models\n",
    "    for model_path in models_paths:\n",
    "        # Load the model\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model = checkpoint['model_state_dict']\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Evaluate the model using the evaluate_model function\n",
    "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "        # Append the accuracy and loss to the lists\n",
    "        accuracies.append(test_accuracy)\n",
    "        losses.append(test_loss)\n",
    "        \n",
    "    # Plot the accuracy and loss for each model\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Plot the accuracy\n",
    "    axs[0].barh(range(len(models_paths)), accuracies, color='skyblue')\n",
    "    axs[0].set_yticks(range(len(models_paths)))\n",
    "    axs[0].set_yticklabels([model_path.split('/')[-2] for model_path in models_paths])\n",
    "    axs[0].set_xlabel('Accuracy')\n",
    "    axs[0].set_title('Model Accuracy Comparison')\n",
    "\n",
    "    # Plot the loss\n",
    "    axs[1].barh(range(len(models_paths)), losses, color='salmon')\n",
    "    axs[1].set_yticks(range(len(models_paths)))\n",
    "    axs[1].set_yticklabels([model_path.split('/')[-2] for model_path in models_paths])\n",
    "    axs[1].set_xlabel('Loss')\n",
    "    axs[1].set_title('Model Loss Comparison')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print the accuracy and loss for each model\n",
    "    for model_path, accuracy, loss in zip(models_paths, accuracies, losses):\n",
    "        print(f\"Model: {model_path.split('/')[-2]}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(f\"Loss: {loss:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    return \n",
    "\n",
    "# Define the paths of the models\n",
    "models_paths = [\n",
    "    './Model/Simple-CNN/simple-model-best.pth',\n",
    "    './Model/Improved-CNN/improved-model-best.pth',\n",
    "    './Model/Deep-CNN/deep-model-best.pth',\n",
    "    './Model/VGG16/vgg16-bn-model-best.pth',\n",
    "    './Model/VGG16/vgg16-1-model-best.pth',\n",
    "    './Model/VGG16/vgg16-2-model-best.pth'\n",
    "]\n",
    "\n",
    "# Assuming test_loader and device are already defined\n",
    "compare_models(models_paths, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cce3207-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0fc60f28c58b4a7da0b51dac730625c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_587bc54298d947b0bbdbb5f902f17118",
        "IPY_MODEL_4261655e150f44ec829fba7844e71fa7",
        "IPY_MODEL_12bfe139c804470e92f151b50fcd7efe"
       ],
       "layout": "IPY_MODEL_f43210083ffd424c8902859bc67ba3fa",
       "tabbable": null,
       "tooltip": null
      }
     },
     "12bfe139c804470e92f151b50fcd7efe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2b93a24f03a44dcebc79a73415e36efa",
       "placeholder": "​",
       "style": "IPY_MODEL_6f9dcbbc3af640e9b0870e6e0616eba4",
       "tabbable": null,
       "tooltip": null,
       "value": " 50/50 [2:16:03&lt;00:00, 158.58s/it, Loss: 0.1433, Train Acc: 0.96%, Val Acc: 0.54%]"
      }
     },
     "2aae2e0705cb4ea882ffda657a2c0e05": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2b93a24f03a44dcebc79a73415e36efa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4261655e150f44ec829fba7844e71fa7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6722c123e2934db1bd2df83649157bbb",
       "max": 50,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2aae2e0705cb4ea882ffda657a2c0e05",
       "tabbable": null,
       "tooltip": null,
       "value": 50
      }
     },
     "4e51e9ba29ed42a086aed2b7017a34fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "587bc54298d947b0bbdbb5f902f17118": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4e51e9ba29ed42a086aed2b7017a34fe",
       "placeholder": "​",
       "style": "IPY_MODEL_7f755cfd5f6343c1b5836dcc7a5efcd0",
       "tabbable": null,
       "tooltip": null,
       "value": "Training Progress: 100%"
      }
     },
     "6722c123e2934db1bd2df83649157bbb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f9dcbbc3af640e9b0870e6e0616eba4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7f755cfd5f6343c1b5836dcc7a5efcd0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f43210083ffd424c8902859bc67ba3fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
