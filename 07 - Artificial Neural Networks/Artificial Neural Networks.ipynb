{"cells":[{"cell_type":"markdown","id":"a3536fb3","metadata":{"id":"a3536fb3"},"source":["### Artificial Neural Networks\n","\n","This experiment will be used to train a simple neural network architecture using pytorch. This network will be trained for character recognition using the MNIST dataset where all the images have a size of 28x28. In this experiment we will consider that the input image are flattened from an array of 28x28 to a vector of 784. The neural network architecture to be used is given below, where it consists of 784 input neurons, 128 neurons in the first hidden layer, 64 neurons in the second hidden layer and ten neurons in the output layer.\n","\n","![Simple Neural Network Architecture](https://miro.medium.com/max/2400/1*HWhBextdDSkxYvz0kEMTVg.png)\n","\n","We start by defining a network class as shown in the following cell.\n"]},{"cell_type":"code","execution_count":null,"id":"1743855f","metadata":{"id":"1743855f"},"outputs":[],"source":["from torch import nn\n","class Network(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Inputs to hidden layer linear transformation\n","        self.hidden1 = nn.Linear(784, 128)\n","        self.hidden2 = nn.Linear(128, 64)\n","        # Output layer, 10 units - one for each digit\n","        self.output = nn.Linear(64, 10)\n","\n","        # Define sigmoid activation and softmax output\n","        self.sigmoid = nn.Sigmoid()\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        # Pass the input tensor through each of our operations\n","        x = self.hidden1(x)\n","        x = self.sigmoid(x)\n","        x = self.hidden2(x)\n","        x = self.sigmoid(x)\n","        x = self.output(x)\n","        x = self.softmax(x)\n","\n","        return x"]},{"cell_type":"markdown","id":"48e0e995","metadata":{"id":"48e0e995"},"source":["Let’s go through this line by line.\n","\n","```python\n","class Network(nn.Module):\n","```\n","\n","Here we’re inheriting from ```nn.Module```. Combined with ```super().__init__()``` this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from ```nn.Module``` when you're creating a class for your network. The name of the class itself can be anything.\n","\n","```python\n","self.hidden1 = nn.Linear(784, 128)\n","self.hidden2 = nn.Linear(128, 64)\n","```\n","\n","This line creates a module for a linear transformation with 784 inputs and 128 outputs, followed by another hidden layer with 128 inputs and 64 outputs assigned to ```self.hidden1``` and ```self.hidden2``` respectively. The module automatically creates the weight and bias tensors which we'll use in the forward method. You can access the weight and bias tensors once the network (```net```) is created with net.hidden.weight and net.hidden.bias.\n","\n","```python\n","self.output = nn.Linear(64, 10)\n","```\n","\n","Similarly, this creates another linear transformation with 256 inputs and 10 outputs.\n","\n","```python\n","self.sigmoid = nn.Sigmoid()\n","self.softmax = nn.Softmax(dim=1)\n","```\n","\n","Here we defined operations for the sigmoid activation and softmax output. Setting dim=1 in ```nn.Softmax(dim=1)``` calculates softmax across the columns.\n","\n","```python\n","def forward(self, x):\n","```\n","\n","PyTorch networks created with ```nn.Module``` must have a ```forward``` method defined. It takes in a tensor ```x``` and passes it through the operations you defined in the ```__init__``` method.\n","\n","```python\n","x = self.hidden1(x)\n","x = self.hidden2(x)\n","x = self.output(x)\n","x = self.softmax(x)\n","```\n","\n","Here the input tensor ```x``` is passed through each operation and reassigned to ```x```. We can see that the input tensor goes through the hidden layer, then a sigmoid function, then the output layer, and finally the softmax function. It doesn't matter what you name the variables here, as long as the inputs and outputs of the operations match the network architecture you want to build. The order in which you define things in the ```__init__``` method doesn't matter, but you'll need to sequence the operations correctly in the forward method."]},{"cell_type":"code","execution_count":null,"id":"2b015d8d","metadata":{"id":"2b015d8d"},"outputs":[],"source":["# Create the network and look at it's text representation\n","model = Network()\n","print(model)"]},{"cell_type":"markdown","id":"99b8cf83","metadata":{"id":"99b8cf83"},"source":["## Building Neural Network using ```nn.Sequential```\n","\n","PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, ```nn.Sequential```([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build a neural network this time with two hidden layers:\n"]},{"cell_type":"code","execution_count":null,"id":"dcaa2ec9","metadata":{"id":"dcaa2ec9"},"outputs":[],"source":["# Hyperparameters for our network\n","input_size = 784\n","hidden_sizes = [128, 64]\n","output_size = 10\n","# Build a feed-forward network\n","model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n","                      nn.ReLU(),\n","                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n","                      nn.ReLU(),\n","                      nn.Linear(hidden_sizes[1], output_size),\n","                      nn.Softmax(dim=1))\n","print(model)"]},{"cell_type":"markdown","id":"cff89d7d","metadata":{"id":"cff89d7d"},"source":["## Training the Neural Network\n","\n","First we start by downloading the MNIST dataset which will be used for both training and testing. In this experiment we will use a batch size of 64."]},{"cell_type":"code","execution_count":null,"id":"84343fb6","metadata":{"id":"84343fb6"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","\n","# Define a transform to normalize the data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize((0.5,), (0.5,)), # (img - mean) / std\n","                              ])\n","# Download and load the training data\n","trainset = datasets.MNIST('./Data/Training/', download=True, train=True, transform=transform)\n","testset  = datasets.MNIST('./Data/Testing/', download=True, train=False, transform=transform)\n","\n","# Prepare the train and test loader\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n","testloader  = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"]},{"cell_type":"markdown","id":"9c3b3619","metadata":{"id":"9c3b3619"},"source":["Now lets visualize a bit the data and see the ground truth."]},{"cell_type":"code","execution_count":null,"id":"67699c05","metadata":{"id":"67699c05"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","examples = enumerate(testloader)\n","batch_idx, (example_data, example_targets) = next(examples)\n","\n","fig = plt.figure()\n","for i in range(6):\n","  plt.subplot(2,3,i+1)\n","  plt.tight_layout()\n","  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n","  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n","  plt.xticks([])\n","  plt.yticks([])\n","fig"]},{"cell_type":"markdown","id":"139ae18c","metadata":{"id":"139ae18c"},"source":["Some nomenclature, one pass through the entire dataset is called an epoch. So here we’re going to loop through ```trainloader``` to get our training batches. For each batch, we'll doing a training pass where we calculate the loss, do a backwards pass, and update the weights. The performance of the network is expected to improve after each epoch until it saturates. So we are going to print the training loss after each epoch."]},{"cell_type":"code","execution_count":null,"id":"86fec3e6","metadata":{"id":"86fec3e6"},"outputs":[],"source":["import tqdm\n","\n","# Defining the network to be trained\n","model = nn.Sequential(nn.Linear(784, 128),\n","                      nn.ReLU(),\n","                      nn.Linear(128, 64),\n","                      nn.ReLU(),\n","                      nn.Linear(64, 10),\n","                      nn.LogSoftmax(dim=1))\n","\n","# Define the loss\n","criterion = nn.NLLLoss()\n","\n","# Optimizers require the parameters to optimize and a learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.003)\n","epochs = 5\n","\n","for e in tqdm.notebook.tqdm(range(epochs)):\n","    running_loss = 0\n","    for images, labels in trainloader:\n","        # Flatten MNIST images into a 784 long vector\n","        images = images.view(images.shape[0], -1)\n","\n","        # Training pass\n","        optimizer.zero_grad()\n","\n","        output = model(images)\n","        loss = criterion(output, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","    else:\n","        print(f\"Training loss: {running_loss/len(trainloader)}\")"]},{"cell_type":"markdown","id":"509540ba","metadata":{"id":"509540ba"},"source":["Note: ```optimizer.zero_grad()``` : When we do multiple backwards passes with the same parameters, the gradients are accumulated. This means that we need to zero the gradients on each training pass or we'll retain gradients from previous training batches.\n","\n","Now it is time to evaluate the performance on the testing data. This model achieves an accuracy of around 89-90%."]},{"cell_type":"code","execution_count":null,"id":"74c82d2f","metadata":{"id":"74c82d2f"},"outputs":[],"source":["model.eval()\n","test_loss = 0\n","correct = 0\n","test_losses = []\n","with torch.no_grad():\n","    for images, target in testloader:\n","\n","        # Flatten MNIST images into a 784 long vector\n","        images = images.view(images.shape[0], -1)\n","\n","        output = model(images)\n","\n","        test_loss += F.nll_loss(output, target, size_average=False).item()\n","\n","        pred = output.data.max(1, keepdim=True)[1]\n","\n","        prediction = pred[0]\n","\n","        correct += pred.eq(target.data.view_as(pred)).sum()\n","\n","    test_loss /= len(testloader.dataset)\n","    test_losses.append(test_loss)\n","    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, len(testloader.dataset),\n","        100. * correct / len(testloader.dataset)))"]},{"cell_type":"markdown","id":"c952a736","metadata":{"id":"c952a736"},"source":["Now we will evaluate the performance of the network on the testing data and use some visualization. We first plot a batch as a 8x8 grid and display the ground truth and also the predictions for the batch. We also print the accuracy of the considered batch."]},{"cell_type":"code","execution_count":null,"id":"a61c363d","metadata":{"id":"a61c363d"},"outputs":[],"source":["import torchvision\n","import numpy as np\n","\n","def imshow(img):\n","    img = (img * 0.5) + 0.5     # Unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","dataiter = iter(testloader)\n","images, labels = dataiter.next()\n","\n","# Print images\n","imshow(torchvision.utils.make_grid(images))\n","\n","print('Ground-Truth')\n","gt = np.array([int(labels[j])for j in range(64)]).reshape((8,8))\n","print(gt)\n","\n","# Flatten MNIST images into a 784 long vector\n","images = images.view(images.shape[0], -1)\n","outputs = model(images)\n","\n","_, predicted = torch.max(outputs, 1)\n","\n","print('Predicted')\n","pred = np.array([int(predicted[j])for j in range(64)]).reshape((8,8))\n","print(pred)\n","\n","correct = np.sum(pred.flatten() == gt.flatten())\n","print('Accuracy: {}'.format(float(correct)/64))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}